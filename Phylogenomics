159 (151 genbank, 8 refseq, these are duplicates, will need to remove) reference nematode genome assemblies were taken from NCBI genbank. Only included sequences at the scaffold level or above from the 2010 or newer.

# copy fna files from downloaded species folders and put in single flder for busco 
find ./* -type f -exec cp -n {} . \;

# To parralellize this and save time, I split the 159 genomes into 10 (15-16 genomes per folder) separate folders, then looped busco on each of them

for f1 in *.fna
do
busco -c 24 -i $f1 -l nematoda_odb10 --augustus -o ${f1%%.fna}"" -m geno
done
# takes a very long time to run busco on this many

# Next needed an outgroup
# Previously used out groups for neamtoda include kinorhyncha, priaplulida, tardigrada, nematomorpha
# there are no nematomorpha, kinorhyncha, or nematomorpha genomes available
# there are 5 targigrade and 1 priapulida reference genomes available
# Going to try them as an outgroup
# They were downloaded and run against the nematoda_odb10 database, as above.

# My next task was to decide on a cutoff for busco quality. Some of the busco scores of the assemblies were low
# This paper "A Practical Guide to Design and Assess a Phylogenomic Study" indicates a cutoff of 80% could be valid
# However it is known the nematoda_odb10 has some bias towards certain clades, and against tylenchina species, so it makes sense to lower this threshold given that knowledge
# Here I checked and removed the buscos lower than 65% from the directory
for dir in /srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/ncbi_dataset/data/FinishedBuscoFolders/*/; do
    for file in "$dir"/*.txt; do
        species=$(basename "$dir")
        echo -n "$species $(grep 'C:' "$file" | awk '{print $1, $2}') " >> output.txt
    done
    echo "" >> output.txt
done
sed 's/^.*C:\([0-9.]*%\).*$/\1/' output.txt > BuscoScores.txt
cut -f 1 output.txt > AssemblyNames.txt
paste AssemblyNames.txt BuscoScores.txt > FinalBuscoScores.txt
# I then used excel to see which are less than 65% and generated a list of them called LowBuscoScores.csv
mkdir LowBuscos
cut -d ',' -f 1 LowBuscoScores.csv | tr -d '\r' | xargs -I {} mv {} LowBuscos/ # this will move the low scoring busco folders to a folder called LowBuscoScores

#It is also worth noting i cannot apply this 70% cutoff to the outgroup, because it wont have very high scores compared to nematoda_odb10, but is needed for the analysis.

# Once I have the single copy busco amino acids from the species set that I want, next i needed to get all the single copy genes from every species,
    #rename them accordingly, then put them all in a directory together for further analysis

# Loop through original subdirectories
for dir in */; do
    # Change to single_copy_busco_sequences subdirectory
    cd $dir/run_nematoda_odb10/busco_sequences/single_copy_busco_sequences
    # Get abbreviation for directory name
    abbrv=$(basename $dir)
    # Make renamed directory
    mkdir renamed
    # Loop through .faa files in single_copy_busco_sequences directory
    for file in *.faa; do
        # Copy file to renamed directory with abbreviation in name
        cp $file renamed/${abbrv}_${file}
        # Replace > with >$abbrv| in copied file
        sed -i "s/^>/>$abbrv|/" renamed/${abbrv}_${file}
        # Convert sequence names to upper case and write to new file
        cut -f 1 -d ":" renamed/${abbrv}_${file} | tr '[:lower:]' '[:upper:]' > renamed/${abbrv}_${file}.1
        # Move new file to original file name
        mv renamed/${abbrv}_${file}.1 renamed/${abbrv}_${file}
    done
    # Move everything to foldder for further analysis
    mv renamed/* /srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/SingleCopyAminoAcids
    # Change back to original directory
    cd ../../../../
done

# The next step is to create a list of the peptides and which species has each of them
# This is done because I want to exclude peptides that are not present in at least 2 different species

# First must copy each full_table.tsv file from each species into a new folder, and name each of them 
#create list of peptides
First need to get the busco files in a folder, with only those which are included (i.e. not too low busco score).
ls SingleCopyAminoAcid > list.txt
sed -i 's/\(.*genomic\).*/\1/' list.txt # remove the amino acid names
sed -i 's/\(.*RhabdiasPseudo\).*/\1/' list.txt # removes the amino acid names from Rpseudo busco
sort list.txt | uniq > list_unique.txt # remove duplicates
# list_unique.txt now contains tha names of the busco files we need
# Now we move the busco folders to a new directory which contains only the Busco folders in the final analysis
xargs -a list_unique.txt -I {} sh -c 'mkdir -p usedBuscos && mv {} usedBuscos' sh

# Now I will extract the full_table.tsv files from each busco folder, rename them, and copy them to a new file
root_dir="/srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/ncbi_dataset/data/FinishedBuscoFolders/usedBuscos"
# create the new directory to move the files to
mkdir tables

# loop through each subdirectory
for subdir in "${root_dir}"/*; do
  # check if the subdirectory has the required subdirectory and file
  if [ -d "${subdir}/run_nematoda_odb10" ] && [ -f "${subdir}/run_nematoda_odb10/full_table.tsv" ]; then
    # get the name of the subdirectory
    subdir_name=$(basename "${subdir}")
    # copy the file to the new directory with the appropriate name
    cp "${subdir}/run_nematoda_odb10/full_table.tsv" "tables/${subdir_name}.tsv"
  fi
done

# This code will collate the tsvs, then print a list of the peptides that are present in more than 2 species
for file in *.tsv
do
grep -v "^#" ${file} | awk '$2=="Complete" {print $1}' >> complete_busco_ids.txt
done
# take only uniq sequences
sort complete_busco_ids.txt |uniq -c > complete_busco_ids_with_counts.txt
awk '$NF >  0 {print $2}' complete_busco_ids_with_counts.txt >  final_busco_ids.txt
# But this code was irrelevant because none had less than 17

# This code is very important, it makes multifasta for each gene, with each multifasta containing the correct sequence from each peptide
# With the output from this, we can begin to align and build trees
cd SingleCopyAminoAcids
while read line; do
cat *_${line}.faa > /srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/FinalSingleCopy/${line}_aa.fasta;
done<final_busco_ids.txt

# next came alignment and tree building for each sequence. for this we can use haqesac, a slimsuite tool
Here we can reuse the environment made for diploidocus, because it is a slimsuite tool too
conda activate dip_env
cd /srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/FinalSingleCopy/9aa
SLIM=/srv/scratch/z5228384/SLiMSuite/tools
module add blast-plus/2.12.0 clustalw clustal-omega fasttree/2.1.11

# Have to change header name length for blast in haqesac
# Trim everything but the species identifier and put them in a new folder
mkdir HeadersShort
for file in *.fasta
do
sed -E 's/^>([^_]+_[^_]+).*/>\1/' $file > HeadersShort/${file%%.fasta}"_shortheader.fasta"
done
cd HeadersShort

# I want a cleaned alignment from haqesac, but I dont need a tree from it
# I will build trees in iqtree2, I dont want to use fasttree, which is haqesacs way of tree building
# specify gnspacc=F to stop it lengthening the species names and preventing blast from working
# specify seqnr=F to stop it from removing sequences that are too similar. We dont want to do that here
# specify unkspec=T so it doesnt skip unknown sequences
# it will claim to have errors after each alignment, but this is only due to the fact we are not building trees and they can be ignored
# It still prodices the cleaned alignment files despite these errors
for FAA in *.fasta
do
PREFIX=$(basename ${FAA/.fasta/})
python2 $SLIM/haqesac.py -seqin $FAA gnspacc=F seqnr=F -basefile $PREFIX forks=8 i=-1 group=dup qryvar=T unkspec=T root=mid query=1
done
# we can then proceed with the .haq.fas alignment files

# Lets move them all into the same directory
# this code is run for each directory 
cp /srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/FinalSingleCopy/9aa/HeadersShort/HAQESAC/*.haq.fas /srv/scratch/z5228384/RhabGenomePaper/Phylogenomics/HAQESAC

#remove alignments with less than 3 sequences
for file in *.fas
do
grep -o '>' $file | wc -l
done > metadata/seqs.txt
for file in *.fas
do
echo $file
done > metadata/alignments.txt
paste -d ': '  alignments.txt /dev/null seqs.txt > result.txt
# to check the number of sequences in alignments
sort -k2 result.txt | head
# 9998at6231_aa_shortheader.haq.fas was the onnly alignment removed (it had one), all others had 10 or above

#with files in the same folder folder
#create reference tree
iqtree2 -p ./400min --prefix concat -bi 100 -B 1000 -T 24
#generate gene trees
iqtree2 -S ./400min --prefix gene -T 24
#generate gene concordance factors
iqtree2 -t concat.treefile --gcf gene.treefile --prefix concord -T 24
#generate site concordance factors
tail concat.log #get model parameters from reference tree
iqtree2 -te concat.treefile -s ./400min -T 24 --scf 100 --prefix concord2 -blfix -m "Q.plant+I{0.177536}+R8{0.147295,0.0935335,0.114418,0.190578,0.108376,0.538389,0.113777,0.804005,0.0898871,1.30004,0.137297,1.95653,0.0958285,3.48597,0.0155849,6.09904}"
-te concat.treefile -s ./400min -T 24 --scfl 100 --prefix scfconcord/iqtree2 -te concat.treefile -s ./400min -T 24 --scfl 100 --prefix scfconcord


# parallelise modelfinder for IQTREE
# 
mkdir output
find *.fasta | parallel --bar 'iqtree -s {} -m MF -msub nuclear -pre output/{}'  
